"""
Syst√®me d'√âv√©nements Temps R√©el pour GNS3 Central Service.

Ce module impl√©mente un syst√®me complet d'√©v√©nements temps r√©el utilisant :
- WebSocket pour la communication bidirectionnelle
- Redis Pub/Sub pour la distribution d'√©v√©nements
- Channels Django pour la gestion WebSocket
- Queue syst√®me pour la gestion des √©v√©nements asynchrones
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable, Set
from dataclasses import dataclass, asdict
from enum import Enum
from concurrent.futures import ThreadPoolExecutor

from django.core.cache import cache
from django.utils import timezone
from django.conf import settings
from channels.generic.websocket import AsyncWebsocketConsumer
from channels.db import database_sync_to_async
from redis import Redis
import redis.asyncio as aioredis

from .gns3_central_service import GNS3Event, GNS3EventType
from .ubuntu_notification_service import ubuntu_notification_service

logger = logging.getLogger(__name__)


class EventPriority(Enum):
    """Priorit√©s des √©v√©nements pour la gestion de la queue."""
    CRITICAL = "critical"
    HIGH = "high" 
    NORMAL = "normal"
    LOW = "low"


class EventDeliveryStatus(Enum):
    """Statuts de livraison des √©v√©nements."""
    PENDING = "pending"
    DELIVERED = "delivered"
    FAILED = "failed"
    RETRY = "retry"


@dataclass
class RealtimeEvent:
    """√âv√©nement temps r√©el enrichi."""
    event_id: str
    event_type: str
    source: str
    data: Dict[str, Any]
    timestamp: datetime
    priority: EventPriority = EventPriority.NORMAL
    delivery_status: EventDeliveryStatus = EventDeliveryStatus.PENDING
    retry_count: int = 0
    max_retries: int = 3
    target_modules: Optional[List[str]] = None
    correlation_id: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convertit l'√©v√©nement en dictionnaire."""
        return {
            'event_id': self.event_id,
            'event_type': self.event_type,
            'source': self.source,
            'data': self.data,
            'timestamp': self.timestamp.isoformat(),
            'priority': self.priority.value,
            'delivery_status': self.delivery_status.value,
            'retry_count': self.retry_count,
            'correlation_id': self.correlation_id,
            'target_modules': self.target_modules or []
        }


class GNS3WebSocketConsumer(AsyncWebsocketConsumer):
    """
    Consommateur WebSocket pour les √©v√©nements GNS3 temps r√©el.
    
    G√®re les connexions WebSocket des clients et distribue les √©v√©nements
    en fonction des abonnements de chaque connexion.
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.room_group_name = "gns3_events"
        self.user_subscriptions: Set[str] = set()
        self.connection_id = str(uuid.uuid4())
        self.last_heartbeat = timezone.now()
        
    async def connect(self):
        """Accepte la connexion WebSocket."""
        try:
            # Rejoindre le groupe d'√©v√©nements GNS3
            await self.channel_layer.group_add(
                self.room_group_name,
                self.channel_name
            )
            
            await self.accept()
            
            # Enregistrer la connexion
            await self._register_connection()
            
            # Envoyer un message de bienvenue
            await self.send(text_data=json.dumps({
                'type': 'connection_established',
                'connection_id': self.connection_id,
                'message': 'Connexion WebSocket GNS3 √©tablie',
                'available_subscriptions': [
                    'node_status', 'topology_changes', 'project_events', 'all_events'
                ],
                'timestamp': timezone.now().isoformat()
            }))
            
            logger.info(f"Connexion WebSocket GNS3 √©tablie: {self.connection_id}")
            
        except Exception as e:
            logger.error(f"Erreur lors de la connexion WebSocket: {e}")
            await self.close()
    
    async def disconnect(self, close_code):
        """Ferme la connexion WebSocket."""
        try:
            # Quitter le groupe d'√©v√©nements
            await self.channel_layer.group_discard(
                self.room_group_name,
                self.channel_name
            )
            
            # D√©senregistrer la connexion
            await self._unregister_connection()
            
            logger.info(f"Connexion WebSocket GNS3 ferm√©e: {self.connection_id} (code: {close_code})")
            
        except Exception as e:
            logger.error(f"Erreur lors de la fermeture WebSocket: {e}")
    
    async def receive(self, text_data):
        """Re√ßoit et traite les messages du client."""
        try:
            data = json.loads(text_data)
            message_type = data.get('type')
            
            if message_type == 'subscribe':
                await self._handle_subscription(data)
            elif message_type == 'unsubscribe':
                await self._handle_unsubscription(data)
            elif message_type == 'heartbeat':
                await self._handle_heartbeat(data)
            elif message_type == 'request_topology':
                await self._handle_topology_request()
            elif message_type == 'node_action':
                await self._handle_node_action(data)
            else:
                await self.send(text_data=json.dumps({
                    'type': 'error',
                    'message': f'Type de message non reconnu: {message_type}'
                }))
                
        except json.JSONDecodeError:
            await self.send(text_data=json.dumps({
                'type': 'error',
                'message': 'Format JSON invalide'
            }))
        except Exception as e:
            logger.error(f"Erreur lors du traitement du message WebSocket: {e}")
            await self.send(text_data=json.dumps({
                'type': 'error',
                'message': f'Erreur interne: {str(e)}'
            }))
    
    async def _handle_subscription(self, data):
        """G√®re les demandes d'abonnement."""
        subscriptions = data.get('subscriptions', [])
        
        for subscription in subscriptions:
            if subscription in ['node_status', 'topology_changes', 'project_events', 'all_events']:
                self.user_subscriptions.add(subscription)
        
        await self.send(text_data=json.dumps({
            'type': 'subscription_confirmed',
            'subscriptions': list(self.user_subscriptions),
            'message': f'Abonn√© √† {len(self.user_subscriptions)} types d\'√©v√©nements'
        }))
        
        # Mettre √† jour le cache des abonnements
        await self._update_subscription_cache()
    
    async def _handle_unsubscription(self, data):
        """G√®re les demandes de d√©sabonnement."""
        subscriptions = data.get('subscriptions', [])
        
        for subscription in subscriptions:
            self.user_subscriptions.discard(subscription)
        
        await self.send(text_data=json.dumps({
            'type': 'unsubscription_confirmed',
            'remaining_subscriptions': list(self.user_subscriptions)
        }))
        
        await self._update_subscription_cache()
    
    async def _handle_heartbeat(self, data):
        """G√®re les messages de heartbeat."""
        self.last_heartbeat = timezone.now()
        
        await self.send(text_data=json.dumps({
            'type': 'heartbeat_ack',
            'server_time': self.last_heartbeat.isoformat()
        }))
    
    async def _handle_topology_request(self):
        """G√®re les demandes de topologie."""
        from .gns3_central_service import gns3_central_service
        
        try:
            topology = gns3_central_service.get_cached_topology()
            
            await self.send(text_data=json.dumps({
                'type': 'topology_response',
                'data': topology or {},
                'timestamp': timezone.now().isoformat()
            }))
        except Exception as e:
            await self.send(text_data=json.dumps({
                'type': 'error',
                'message': f'Erreur lors de la r√©cup√©ration de la topologie: {str(e)}'
            }))
    
    async def _handle_node_action(self, data):
        """G√®re les actions sur les n≈ìuds."""
        from .gns3_central_service import gns3_central_service
        
        try:
            action = data.get('action')
            project_id = data.get('project_id')
            node_id = data.get('node_id')
            
            if not all([action, project_id, node_id]):
                await self.send(text_data=json.dumps({
                    'type': 'error',
                    'message': 'action, project_id et node_id sont requis'
                }))
                return
            
            if action == 'start':
                result = await gns3_central_service.start_node(project_id, node_id)
            elif action == 'stop':
                result = await gns3_central_service.stop_node(project_id, node_id)
            elif action == 'restart':
                result = await gns3_central_service.restart_node(project_id, node_id)
            else:
                await self.send(text_data=json.dumps({
                    'type': 'error',
                    'message': f'Action non support√©e: {action}'
                }))
                return
            
            await self.send(text_data=json.dumps({
                'type': 'node_action_result',
                'action': action,
                'project_id': project_id,
                'node_id': node_id,
                'result': result
            }))
            
        except Exception as e:
            await self.send(text_data=json.dumps({
                'type': 'error',
                'message': f'Erreur lors de l\'action sur le n≈ìud: {str(e)}'
            }))
    
    async def _register_connection(self):
        """Enregistre la connexion dans le cache."""
        connection_data = {
            'connection_id': self.connection_id,
            'channel_name': self.channel_name,
            'connected_at': timezone.now().isoformat(),
            'subscriptions': list(self.user_subscriptions),
            'last_heartbeat': self.last_heartbeat.isoformat()
        }
        
        cache.set(f"gns3_websocket_connection:{self.connection_id}", connection_data, timeout=3600)
    
    async def _unregister_connection(self):
        """D√©senregistre la connexion du cache."""
        cache.delete(f"gns3_websocket_connection:{self.connection_id}")
    
    async def _update_subscription_cache(self):
        """Met √† jour le cache des abonnements."""
        connection_data = cache.get(f"gns3_websocket_connection:{self.connection_id}")
        if connection_data:
            connection_data['subscriptions'] = list(self.user_subscriptions)
            cache.set(f"gns3_websocket_connection:{self.connection_id}", connection_data, timeout=3600)
    
    # Gestionnaires d'√©v√©nements du group
    async def gns3_event(self, event):
        """Re√ßoit et filtre les √©v√©nements GNS3."""
        event_data = event['event_data']
        event_type = event_data.get('event_type', '')
        
        # Filtrer selon les abonnements
        if self._should_send_event(event_type):
            await self.send(text_data=json.dumps({
                'type': 'gns3_event',
                'event_data': event_data,
                'timestamp': timezone.now().isoformat()
            }))
    
    def _should_send_event(self, event_type: str) -> bool:
        """D√©termine si l'√©v√©nement doit √™tre envoy√© au client."""
        if 'all_events' in self.user_subscriptions:
            return True
        
        # Mapping des types d'√©v√©nements aux abonnements
        event_mapping = {
            'node.started': 'node_status',
            'node.stopped': 'node_status',
            'node.suspended': 'node_status',
            'node.created': 'topology_changes',
            'node.deleted': 'topology_changes',
            'node.updated': 'topology_changes',
            'project.opened': 'project_events',
            'project.closed': 'project_events',
            'project.created': 'project_events',
            'project.deleted': 'project_events',
            'topology.changed': 'topology_changes',
            'link.created': 'topology_changes',
            'link.deleted': 'topology_changes',
        }
        
        required_subscription = event_mapping.get(event_type)
        return required_subscription in self.user_subscriptions


class RealtimeEventManager:
    """
    Gestionnaire d'√©v√©nements temps r√©el pour le service central GNS3.
    
    Responsabilit√©s :
    - Distribution d'√©v√©nements via WebSocket et Redis
    - Gestion des queues d'√©v√©nements par priorit√©
    - Retry logic pour les √©v√©nements √©chou√©s
    - M√©triques et monitoring des √©v√©nements
    """
    
    def __init__(self):
        """Initialise le gestionnaire d'√©v√©nements."""
        self.redis_config = {
            'host': getattr(settings, 'REDIS_HOST', 'localhost'),
            'port': getattr(settings, 'REDIS_PORT', 6379),
            'db': getattr(settings, 'REDIS_DB', 0),
            'decode_responses': True
        }
        
        self.redis_client = Redis(**self.redis_config)
        self.event_queues = {
            EventPriority.CRITICAL: f"gns3_events:critical",
            EventPriority.HIGH: f"gns3_events:high",
            EventPriority.NORMAL: f"gns3_events:normal",
            EventPriority.LOW: f"gns3_events:low"
        }
        
        self.channel_name = "gns3_events_realtime"
        self.statistics = {
            'events_published': 0,
            'events_delivered': 0,
            'events_failed': 0,
            'events_retried': 0,
            'connections_active': 0,
            'last_event_time': None
        }
        
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.is_running = False
        
        logger.info("Gestionnaire d'√©v√©nements temps r√©el GNS3 initialis√©")
    
    async def start(self):
        """D√©marre le gestionnaire d'√©v√©nements."""
        if self.is_running:
            return
        
        self.is_running = True
        
        # D√©marrer les workers de traitement des √©v√©nements
        asyncio.create_task(self._process_event_queues())
        asyncio.create_task(self._cleanup_expired_events())
        asyncio.create_task(self._monitor_connections())
        
        logger.info("Gestionnaire d'√©v√©nements temps r√©el d√©marr√©")
    
    async def stop(self):
        """Arr√™te le gestionnaire d'√©v√©nements."""
        self.is_running = False
        self.executor.shutdown(wait=True)
        logger.info("Gestionnaire d'√©v√©nements temps r√©el arr√™t√©")
    
    async def publish_event(self, event: RealtimeEvent):
        """
        Publie un √©v√©nement vers tous les canaux de distribution.
        
        Args:
            event: √âv√©nement √† publier
        """
        try:
            # G√©n√©rer un ID unique si pas fourni
            if not event.event_id:
                event.event_id = str(uuid.uuid4())
            
            # Ajouter √† la queue Redis selon la priorit√©
            queue_name = self.event_queues[event.priority]
            event_data = json.dumps(event.to_dict())
            
            await self._async_redis_lpush(queue_name, event_data)
            
            # Publier imm√©diatement via Redis Pub/Sub pour les √©v√©nements critiques
            if event.priority in [EventPriority.CRITICAL, EventPriority.HIGH]:
                await self._publish_to_pubsub(event)
            
            # Publier via Channels pour WebSocket
            await self._publish_to_websocket(event)
            
            # Mettre √† jour les statistiques
            self.statistics['events_published'] += 1
            self.statistics['last_event_time'] = timezone.now()
            
            # Cache pour la tra√ßabilit√©
            cache.set(f"gns3_event:{event.event_id}", event.to_dict(), timeout=3600)
            
            logger.debug(f"√âv√©nement publi√©: {event.event_type} (ID: {event.event_id})")
            
        except Exception as e:
            logger.error(f"Erreur lors de la publication d'√©v√©nement: {e}")
            raise
    
    async def publish_gns3_event(self, gns3_event: GNS3Event):
        """
        Publie un √©v√©nement GNS3 en le convertissant en √©v√©nement temps r√©el.
        
        Args:
            gns3_event: √âv√©nement GNS3 √† publier
        """
        # D√©terminer la priorit√© selon le type d'√©v√©nement
        priority_mapping = {
            GNS3EventType.NODE_STARTED: EventPriority.HIGH,
            GNS3EventType.NODE_STOPPED: EventPriority.HIGH,
            GNS3EventType.NODE_SUSPENDED: EventPriority.NORMAL,
            GNS3EventType.NODE_CREATED: EventPriority.NORMAL,
            GNS3EventType.NODE_DELETED: EventPriority.HIGH,
            GNS3EventType.PROJECT_OPENED: EventPriority.NORMAL,
            GNS3EventType.PROJECT_CLOSED: EventPriority.NORMAL,
            GNS3EventType.TOPOLOGY_CHANGED: EventPriority.HIGH,
        }
        
        priority = priority_mapping.get(gns3_event.event_type, EventPriority.NORMAL)
        
        realtime_event = RealtimeEvent(
            event_id=str(uuid.uuid4()),
            event_type=gns3_event.event_type.value,
            source=gns3_event.source,
            data=gns3_event.data,
            timestamp=gns3_event.timestamp,
            priority=priority,
            correlation_id=gns3_event.data.get('correlation_id')
        )
        
        await self.publish_event(realtime_event)
    
    async def _publish_to_pubsub(self, event: RealtimeEvent):
        """Publie l'√©v√©nement via Redis Pub/Sub."""
        try:
            redis_async = aioredis.from_url(f"redis://{self.redis_config['host']}:{self.redis_config['port']}")
            
            channel = f"gns3_events:{event.event_type}"
            await redis_async.publish(channel, json.dumps(event.to_dict()))
            
            await redis_async.close()
            
        except Exception as e:
            logger.error(f"Erreur lors de la publication Redis Pub/Sub: {e}")
    
    async def _publish_to_websocket(self, event: RealtimeEvent):
        """Publie l'√©v√©nement via WebSocket Channels."""
        try:
            from channels.layers import get_channel_layer
            
            channel_layer = get_channel_layer()
            
            await channel_layer.group_send(
                "gns3_events",
                {
                    "type": "gns3_event",
                    "event_data": event.to_dict()
                }
            )
            
        except Exception as e:
            logger.error(f"Erreur lors de la publication WebSocket: {e}")
    
    async def _async_redis_lpush(self, queue_name: str, data: str):
        """Version async de Redis LPUSH."""
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(self.executor, self.redis_client.lpush, queue_name, data)
    
    async def _process_event_queues(self):
        """Traite les queues d'√©v√©nements par ordre de priorit√©."""
        priorities = [EventPriority.CRITICAL, EventPriority.HIGH, EventPriority.NORMAL, EventPriority.LOW]
        
        while self.is_running:
            try:
                for priority in priorities:
                    queue_name = self.event_queues[priority]
                    
                    # Traiter jusqu'√† 10 √©v√©nements par cycle
                    for _ in range(10):
                        event_data = self.redis_client.rpop(queue_name)
                        if not event_data:
                            break
                        
                        await self._process_single_event(json.loads(event_data))
                
                # Pause entre les cycles
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Erreur dans le traitement des queues d'√©v√©nements: {e}")
                await asyncio.sleep(1)
    
    async def _process_single_event(self, event_data: Dict[str, Any]):
        """Traite un √©v√©nement individuel."""
        try:
            event = RealtimeEvent(**event_data)
            
            # Logique de traitement sp√©cifique
            success = await self._deliver_event(event)
            
            if success:
                event.delivery_status = EventDeliveryStatus.DELIVERED
                self.statistics['events_delivered'] += 1
            else:
                if event.retry_count < event.max_retries:
                    event.retry_count += 1
                    event.delivery_status = EventDeliveryStatus.RETRY
                    self.statistics['events_retried'] += 1
                    
                    # Remettre en queue avec d√©lai
                    await asyncio.sleep(2 ** event.retry_count)  # Backoff exponentiel
                    await self._async_redis_lpush(
                        self.event_queues[event.priority], 
                        json.dumps(event.to_dict())
                    )
                else:
                    event.delivery_status = EventDeliveryStatus.FAILED
                    self.statistics['events_failed'] += 1
                    logger.error(f"√âv√©nement √©chou√© d√©finitivement: {event.event_id}")
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement d'√©v√©nement: {e}")
    
    async def _deliver_event(self, event: RealtimeEvent) -> bool:
        """Livre un √©v√©nement vers ses destinations."""
        try:
            # Notifier les modules int√©ress√©s
            await self._notify_modules(event)
            
            # Notification syst√®me pour √©v√©nements critiques
            if event.priority == EventPriority.CRITICAL:
                ubuntu_notification_service.send_notification(
                    title=f"üö® √âv√©nement GNS3 Critique",
                    message=f"{event.event_type}: {event.data.get('message', '√âv√©nement syst√®me')}",
                    urgency='critical',
                    category='network.gns3'
                )
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de la livraison d'√©v√©nement: {e}")
            return False
    
    async def _notify_modules(self, event: RealtimeEvent):
        """Notifie les modules int√©ress√©s par l'√©v√©nement."""
        # Si des modules cibles sont sp√©cifi√©s
        if event.target_modules:
            for module in event.target_modules:
                await self._send_to_module(module, event)
        else:
            # Sinon, diffuser √† tous les modules abonn√©s
            await self._broadcast_to_all_modules(event)
    
    async def _send_to_module(self, module_name: str, event: RealtimeEvent):
        """Envoie un √©v√©nement √† un module sp√©cifique."""
        try:
            # Utiliser le syst√®me inter-modules existant
            from .inter_module_service import inter_module_service, MessageType
            
            inter_module_service.send_message(
                message_type=MessageType.NETWORK_EVENT,
                data={
                    'gns3_event': event.to_dict(),
                    'source': 'realtime_event_system'
                },
                sender='gns3_realtime_events',
                target_module=module_name
            )
            
        except Exception as e:
            logger.error(f"Erreur lors de l'envoi √† {module_name}: {e}")
    
    async def _broadcast_to_all_modules(self, event: RealtimeEvent):
        """Diffuse un √©v√©nement √† tous les modules."""
        from .inter_module_service import inter_module_service, MessageType
        
        inter_module_service.send_message(
            message_type=MessageType.NETWORK_EVENT,
            data={
                'gns3_event': event.to_dict(),
                'source': 'realtime_event_system'
            },
            sender='gns3_realtime_events'
        )
    
    async def _cleanup_expired_events(self):
        """Nettoie les √©v√©nements expir√©s du cache."""
        while self.is_running:
            try:
                # Nettoyer toutes les heures
                await asyncio.sleep(3600)
                
                # Les √©v√©nements sont automatiquement expir√©s par Redis TTL
                logger.debug("Nettoyage des √©v√©nements expir√©s effectu√©")
                
            except Exception as e:
                logger.error(f"Erreur lors du nettoyage: {e}")
    
    async def _monitor_connections(self):
        """Monitore les connexions WebSocket actives."""
        while self.is_running:
            try:
                # Compter les connexions actives
                pattern = "gns3_websocket_connection:*"
                try:
                    connections = cache.keys(pattern)
                    self.statistics['connections_active'] = len(connections)
                except AttributeError:
                    # LocMemCache n'a pas de m√©thode keys(), utiliser une estimation
                    self.statistics['connections_active'] = getattr(self, '_estimated_connections', 0)
                
                # Nettoyer les connexions expir√©es (seulement si on a acc√®s aux cl√©s)
                if 'connections' in locals():
                    current_time = timezone.now()
                    for conn_key in connections:
                        conn_data = cache.get(conn_key)
                        if conn_data:
                            last_heartbeat = datetime.fromisoformat(conn_data['last_heartbeat'])
                            if current_time - last_heartbeat > timedelta(minutes=5):
                                cache.delete(conn_key)
                
                await asyncio.sleep(30)  # V√©rifier toutes les 30 secondes
                
            except Exception as e:
                logger.error(f"Erreur lors du monitoring des connexions: {e}")
                await asyncio.sleep(60)
    
    def get_statistics(self) -> Dict[str, Any]:
        """R√©cup√®re les statistiques du gestionnaire d'√©v√©nements."""
        return {
            **self.statistics,
            'queue_sizes': {
                priority.value: self.redis_client.llen(queue_name)
                for priority, queue_name in self.event_queues.items()
            },
            'is_running': self.is_running,
            'last_check': timezone.now().isoformat()
        }


# Instance globale du gestionnaire d'√©v√©nements
realtime_event_manager = RealtimeEventManager()


# Fonctions utilitaires pour l'int√©gration
async def publish_gns3_event_realtime(gns3_event: GNS3Event):
    """
    Fonction utilitaire pour publier un √©v√©nement GNS3 en temps r√©el.
    
    Args:
        gns3_event: √âv√©nement GNS3 √† publier
    """
    await realtime_event_manager.publish_gns3_event(gns3_event)


def create_realtime_event(event_type: str, data: Dict[str, Any], 
                         priority: EventPriority = EventPriority.NORMAL,
                         target_modules: Optional[List[str]] = None) -> RealtimeEvent:
    """
    Cr√©e un √©v√©nement temps r√©el.
    
    Args:
        event_type: Type de l'√©v√©nement
        data: Donn√©es de l'√©v√©nement
        priority: Priorit√© de l'√©v√©nement
        target_modules: Modules cibles (optionnel)
        
    Returns:
        √âv√©nement temps r√©el cr√©√©
    """
    return RealtimeEvent(
        event_id=str(uuid.uuid4()),
        event_type=event_type,
        source="gns3_central_service",
        data=data,
        timestamp=timezone.now(),
        priority=priority,
        target_modules=target_modules
    )