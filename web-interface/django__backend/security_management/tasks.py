"""
T√¢ches Celery pour le module Security Management.

Ce module contient les t√¢ches asynchrones pour la d√©tection automatique
des alertes de s√©curit√© et le d√©clenchement des rapports.
"""

import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

from celery import shared_task
from django.utils import timezone
from django.core.cache import cache
from django.conf import settings

logger = logging.getLogger(__name__)


@shared_task(bind=True, max_retries=3, default_retry_delay=30)
def monitor_security_alerts(self):
    """
    T√¢che qui monitore automatiquement Elasticsearch pour les nouvelles alertes Suricata.
    
    Cette t√¢che est ex√©cut√©e toutes les 2 minutes par Celery Beat pour d√©tecter
    les nouvelles alertes de s√©curit√© et d√©clencher automatiquement la g√©n√©ration
    de rapports et l'envoi de notifications.
    """
    try:
        logger.info("üîç D√©marrage monitoring automatique des alertes de s√©curit√©")
        
        from .services.elasticsearch_monitor import ElasticsearchMonitor
        from reporting.tasks import generate_security_report_from_alerts
        from .signals import security_alert_detected
        
        # Initialiser le moniteur Elasticsearch
        es_monitor = ElasticsearchMonitor()
        
        # R√©cup√©rer les nouvelles alertes depuis le dernier scan
        last_scan_key = "security_monitor_last_scan"
        last_scan_time = cache.get(last_scan_key, timezone.now() - timedelta(minutes=2))
        
        # Scanner les nouvelles alertes
        new_alerts = es_monitor.get_new_alerts_since(last_scan_time)
        
        if not new_alerts:
            logger.debug("‚úÖ Aucune nouvelle alerte d√©tect√©e")
            cache.set(last_scan_key, timezone.now(), timeout=3600)
            return {"status": "success", "new_alerts": 0}
        
        logger.warning(f"üö® {len(new_alerts)} nouvelles alertes de s√©curit√© d√©tect√©es!")
        
        # Analyser la criticit√© des alertes
        critical_alerts = []
        high_alerts = []
        medium_alerts = []
        
        for alert in new_alerts:
            severity = alert.get('severity', 'medium').lower()
            
            if severity in ['critical', 'high']:
                if severity == 'critical':
                    critical_alerts.append(alert)
                else:
                    high_alerts.append(alert)
                    
                # √âmettre signal pour alertes critiques/√©lev√©es
                security_alert_detected.send(
                    sender=self.__class__,
                    alert={
                        'id': alert.get('alert_id'),
                        'title': alert.get('signature', 'Alerte de s√©curit√©'),
                        'description': alert.get('message', ''),
                        'severity': severity,
                        'source_ip': alert.get('src_ip'),
                        'destination_ip': alert.get('dest_ip'),
                        'detection_time': alert.get('@timestamp'),
                        'raw_data': alert
                    }
                )
            else:
                medium_alerts.append(alert)
        
        # D√©clencher g√©n√©ration automatique de rapport si alertes critiques/√©lev√©es
        report_triggered = False
        if critical_alerts or high_alerts:
            logger.info(f"üö® D√©clenchement automatique du rapport de s√©curit√©: {len(critical_alerts)} critiques, {len(high_alerts)} √©lev√©es")
            
            # Lancer g√©n√©ration asynchrone du rapport
            report_task = generate_security_report_from_alerts.delay({
                'critical_alerts': critical_alerts,
                'high_alerts': high_alerts,
                'medium_alerts': medium_alerts,
                'detection_time': timezone.now().isoformat(),
                'auto_generated': True,
                'trigger_source': 'security_monitoring'
            })
            
            report_triggered = True
            
            # Sauvegarder info du rapport en cours
            cache.set(f"security_report_task_{report_task.id}", {
                'task_id': report_task.id,
                'started_at': timezone.now().isoformat(),
                'alert_counts': {
                    'critical': len(critical_alerts),
                    'high': len(high_alerts),
                    'medium': len(medium_alerts)
                }
            }, timeout=3600)
        
        # Mettre √† jour m√©triques de monitoring
        cache.set("security_monitoring_stats", {
            'last_scan': timezone.now().isoformat(),
            'alerts_processed': len(new_alerts),
            'critical_alerts': len(critical_alerts),
            'high_alerts': len(high_alerts),
            'medium_alerts': len(medium_alerts),
            'report_triggered': report_triggered,
            'scan_success': True
        }, timeout=86400)
        
        # Mettre √† jour timestamp du dernier scan
        cache.set(last_scan_key, timezone.now(), timeout=3600)
        
        logger.info(f"‚úÖ Monitoring termin√© - {len(new_alerts)} alertes trait√©es, rapport {'d√©clench√©' if report_triggered else 'non requis'}")
        
        return {
            "status": "success",
            "new_alerts": len(new_alerts),
            "critical_alerts": len(critical_alerts),
            "high_alerts": len(high_alerts),
            "medium_alerts": len(medium_alerts),
            "report_triggered": report_triggered
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du monitoring des alertes de s√©curit√©: {e}")
        
        # Mettre √† jour m√©triques d'erreur
        cache.set("security_monitoring_stats", {
            'last_scan': timezone.now().isoformat(),
            'scan_success': False,
            'error': str(e)
        }, timeout=86400)
        
        # Retry avec backoff exponentiel
        try:
            raise self.retry(countdown=30 * (2 ** self.request.retries))
        except self.MaxRetriesExceededError:
            logger.error("‚ö†Ô∏è Nombre maximum de tentatives atteint pour le monitoring de s√©curit√©")


@shared_task
def process_security_event(event_data: Dict[str, Any]):
    """
    Traite un √©v√©nement de s√©curit√© en temps r√©el.
    
    Args:
        event_data: Donn√©es de l'√©v√©nement de s√©curit√©
    """
    try:
        logger.info(f"üîí Traitement √©v√©nement de s√©curit√©: {event_data.get('event_type', 'unknown')}")
        
        # Analyser le type d'√©v√©nement
        event_type = event_data.get('event_type')
        source_ip = event_data.get('source_ip')
        destination_ip = event_data.get('destination_ip')
        severity = event_data.get('severity', 'medium')
        
        # Action automatique selon la s√©v√©rit√©
        if severity == 'critical':
            # Notification imm√©diate pour √©v√©nements critiques
            from .services.notification_service import send_immediate_alert
            send_immediate_alert({
                'title': f"üö® ALERTE CRITIQUE - {event_type}",
                'message': f"Source: {source_ip} ‚Üí Destination: {destination_ip}",
                'event_data': event_data
            })
            
        # Loguer l'√©v√©nement pour corr√©lation future
        from .models import SecurityEvent
        SecurityEvent.objects.create(
            event_type=event_type,
            source_ip=source_ip,
            destination_ip=destination_ip,
            severity=severity,
            event_data=event_data,
            processed_at=timezone.now()
        )
        
        logger.info(f"‚úÖ √âv√©nement de s√©curit√© trait√© avec succ√®s")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur traitement √©v√©nement de s√©curit√©: {e}")


@shared_task
def fetch_suricata_alerts():
    """
    R√©cup√®re les alertes Suricata depuis Elasticsearch.
    
    Cette t√¢che est ex√©cut√©e toutes les 5 minutes pour synchroniser
    les alertes Suricata avec la base de donn√©es Django.
    """
    try:
        logger.info("üì• R√©cup√©ration des alertes Suricata")
        
        from .services.suricata_client import SuricataClient
        
        suricata_client = SuricataClient()
        
        # R√©cup√©rer les alertes depuis le dernier fetch
        last_fetch_key = "suricata_last_fetch"
        last_fetch_time = cache.get(last_fetch_key, timezone.now() - timedelta(minutes=5))
        
        alerts = suricata_client.get_alerts_since(last_fetch_time)
        
        alerts_processed = 0
        for alert_data in alerts:
            try:
                # Cr√©er ou mettre √† jour l'alerte en DB
                from .models import SuricataAlert
                
                alert, created = SuricataAlert.objects.get_or_create(
                    alert_id=alert_data.get('alert_id', alert_data.get('_id')),
                    defaults={
                        'signature': alert_data.get('alert', {}).get('signature', ''),
                        'category': alert_data.get('alert', {}).get('category', ''),
                        'severity': alert_data.get('alert', {}).get('severity', 1),
                        'source_ip': alert_data.get('src_ip', ''),
                        'destination_ip': alert_data.get('dest_ip', ''),
                        'source_port': alert_data.get('src_port', 0),
                        'destination_port': alert_data.get('dest_port', 0),
                        'protocol': alert_data.get('proto', ''),
                        'timestamp': alert_data.get('@timestamp'),
                        'raw_data': alert_data
                    }
                )
                
                if created:
                    alerts_processed += 1
                    
                    # D√©clencher signal pour nouvelle alerte
                    from .signals import security_alert_detected
                    security_alert_detected.send(
                        sender=SuricataAlert,
                        alert=alert
                    )
                    
            except Exception as e:
                logger.error(f"Erreur traitement alerte Suricata: {e}")
                continue
        
        # Mettre √† jour timestamp
        cache.set(last_fetch_key, timezone.now(), timeout=3600)
        
        logger.info(f"‚úÖ {alerts_processed} nouvelles alertes Suricata trait√©es")
        
        return {
            "status": "success",
            "alerts_processed": alerts_processed,
            "total_alerts": len(alerts)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration alertes Suricata: {e}")
        return {
            "status": "error",
            "error": str(e)
        }


@shared_task
def sync_suricata_rules():
    """
    Synchronise les r√®gles Suricata.
    """
    try:
        logger.info("üîÑ Synchronisation des r√®gles Suricata")
        
        from .services.suricata_rules_manager import SuricataRulesManager
        
        rules_manager = SuricataRulesManager()
        result = rules_manager.sync_rules()
        
        if result['success']:
            logger.info(f"‚úÖ R√®gles Suricata synchronis√©es: {result['rules_updated']} mises √† jour")
        else:
            logger.warning(f"‚ö†Ô∏è Probl√®me synchronisation r√®gles: {result.get('error', 'Unknown')}")
            
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erreur synchronisation r√®gles Suricata: {e}")
        return {"success": False, "error": str(e)}


@shared_task
def cleanup_old_security_data():
    """
    Nettoie les anciennes donn√©es de s√©curit√©.
    """
    try:
        logger.info("üßπ Nettoyage des anciennes donn√©es de s√©curit√©")
        
        # Param√®tres de r√©tention
        alert_retention_days = getattr(settings, 'SECURITY_ALERT_RETENTION_DAYS', 30)
        event_retention_days = getattr(settings, 'SECURITY_EVENT_RETENTION_DAYS', 90)
        
        cutoff_alert_date = timezone.now() - timedelta(days=alert_retention_days)
        cutoff_event_date = timezone.now() - timedelta(days=event_retention_days)
        
        # Nettoyer les alertes anciennes
        from .models import SuricataAlert, SecurityEvent
        
        old_alerts = SuricataAlert.objects.filter(timestamp__lt=cutoff_alert_date)
        alerts_deleted = old_alerts.count()
        old_alerts.delete()
        
        # Nettoyer les √©v√©nements anciens
        old_events = SecurityEvent.objects.filter(processed_at__lt=cutoff_event_date)
        events_deleted = old_events.count()
        old_events.delete()
        
        logger.info(f"‚úÖ Nettoyage termin√©: {alerts_deleted} alertes, {events_deleted} √©v√©nements supprim√©s")
        
        return {
            "alerts_deleted": alerts_deleted,
            "events_deleted": events_deleted
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur nettoyage donn√©es de s√©curit√©: {e}")
        return {"error": str(e)}