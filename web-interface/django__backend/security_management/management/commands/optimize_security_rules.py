"""
Commande Django pour l'optimisation sophistiqu√©e des r√®gles de s√©curit√©.

Cette commande effectue :
- L'analyse compl√®te des conflits entre r√®gles
- L'optimisation automatique des performances
- La consolidation des r√®gles redondantes
- L'int√©gration avec les services Docker pour validation
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple

from django.core.management.base import BaseCommand, CommandError
from django.db import transaction
from django.utils import timezone
from django.conf import settings

from ...models import SecurityRuleModel, SecurityAlertModel, AuditLogModel
from ...domain.conflict_detector_factory import conflict_detector_factory
from ...application.detect_rule_conflicts_use_case import DetectRuleConflictsUseCase
from ...domain.impact_analysis import AdvancedRuleMetricsCalculator, DockerMetricsCollector
from ...infrastructure.repositories import (
    DjangoSecurityRuleRepository, DjangoSecurityAlertRepository
)
from ...infrastructure.docker_integration import (
    SuricataDockerAdapter, Fail2BanDockerAdapter, TrafficControlDockerAdapter
)

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    """
    Commande d'optimisation avanc√©e des r√®gles de s√©curit√©.
    
    Usage:
        python manage.py optimize_security_rules [options]
    """
    
    help = 'Optimise les r√®gles de s√©curit√© avec analyse de conflits et int√©gration Docker'
    
    def __init__(self):
        super().__init__()
        # Initialiser les services
        self.rule_repository = DjangoSecurityRuleRepository()
        self.conflict_use_case = DetectRuleConflictsUseCase(
            rule_repository=self.rule_repository,
            conflict_detector_factory=conflict_detector_factory
        )
        self.metrics_calculator = AdvancedRuleMetricsCalculator()
        self.metrics_collector = DockerMetricsCollector()
        
        # Adaptateurs Docker
        self.docker_services = {
            'suricata': SuricataDockerAdapter(),
            'fail2ban': Fail2BanDockerAdapter(),
            'traffic_control': TrafficControlDockerAdapter()
        }
        
        # Statistiques d'optimisation
        self.optimization_stats = {
            'rules_analyzed': 0,
            'conflicts_detected': 0,
            'conflicts_resolved': 0,
            'rules_optimized': 0,
            'rules_consolidated': 0,
            'performance_improved': False,
            'docker_services_validated': 0
        }
    
    def add_arguments(self, parser):
        """Ajoute les arguments de la commande."""
        parser.add_argument(
            '--rule-type',
            choices=['firewall', 'ids', 'access_control', 'all'],
            default='all',
            help='Type de r√®gles √† optimiser (default: all)'
        )
        
        parser.add_argument(
            '--auto-resolve',
            action='store_true',
            help='R√©sout automatiquement les conflits simples'
        )
        
        parser.add_argument(
            '--consolidate',
            action='store_true',
            help='Consolide les r√®gles redondantes'
        )
        
        parser.add_argument(
            '--validate-docker',
            action='store_true',
            help='Valide toutes les r√®gles via les services Docker'
        )
        
        parser.add_argument(
            '--performance-check',
            action='store_true',
            help='Effectue une analyse de performance compl√®te'
        )
        
        parser.add_argument(
            '--dry-run',
            action='store_true',
            help='Simulation sans modifications effectives'
        )
        
        parser.add_argument(
            '--output-format',
            choices=['text', 'json'],
            default='text',
            help='Format de sortie (default: text)'
        )
        
        parser.add_argument(
            '--export-report',
            type=str,
            help='Chemin pour exporter le rapport d\'optimisation'
        )
        
        parser.add_argument(
            '--verbose',
            action='store_true',
            help='Mode verbeux'
        )
    
    def handle(self, *args, **options):
        """Point d'entr√©e principal de la commande."""
        try:
            self.verbosity = options.get('verbosity', 1)
            self.verbose = options.get('verbose', False)
            
            start_time = timezone.now()
            
            if self.verbose:
                self.stdout.write(
                    self.style.SUCCESS(f'üîß D√©marrage de l\'optimisation des r√®gles de s√©curit√© - {start_time}')
                )
            
            # √âtape 1: Analyse des r√®gles existantes
            self._log_step("Analyse des r√®gles existantes")
            rules_to_analyze = self._get_rules_to_analyze(options['rule_type'])
            self.optimization_stats['rules_analyzed'] = len(rules_to_analyze)
            
            if not rules_to_analyze:
                self.stdout.write(
                    self.style.WARNING('‚ö†Ô∏è  Aucune r√®gle trouv√©e pour l\'optimisation')
                )
                return
            
            # √âtape 2: D√©tection des conflits
            self._log_step("D√©tection des conflits")
            conflicts = self._detect_conflicts(rules_to_analyze, options['rule_type'])
            self.optimization_stats['conflicts_detected'] = len(conflicts)
            
            # √âtape 3: Validation Docker (optionnelle)
            if options['validate_docker']:
                self._log_step("Validation via services Docker")
                docker_results = self._validate_rules_via_docker(rules_to_analyze)
                self.optimization_stats['docker_services_validated'] = len(docker_results)
            
            # √âtape 4: Analyse de performance (optionnelle)
            performance_analysis = None
            if options['performance_check']:
                self._log_step("Analyse de performance")
                performance_analysis = self._analyze_performance(rules_to_analyze)
                self.optimization_stats['performance_improved'] = performance_analysis.get('improvements_available', False)
            
            # √âtape 5: R√©solution automatique des conflits (optionnelle)
            if options['auto_resolve'] and conflicts:
                self._log_step("R√©solution automatique des conflits")
                resolved_count = self._auto_resolve_conflicts(conflicts, options['dry_run'])
                self.optimization_stats['conflicts_resolved'] = resolved_count
            
            # √âtape 6: Consolidation des r√®gles (optionnelle)
            if options['consolidate']:
                self._log_step("Consolidation des r√®gles redondantes")
                consolidated_count = self._consolidate_redundant_rules(conflicts, options['dry_run'])
                self.optimization_stats['rules_consolidated'] = consolidated_count
            
            # √âtape 7: Optimisation des performances
            optimization_count = self._optimize_rule_performance(rules_to_analyze, options['dry_run'])
            self.optimization_stats['rules_optimized'] = optimization_count
            
            # G√©n√©ration du rapport
            end_time = timezone.now()
            duration = end_time - start_time
            
            report = self._generate_optimization_report(
                rules_to_analyze, conflicts, performance_analysis, duration
            )
            
            # Affichage des r√©sultats
            self._display_results(report, options['output_format'])
            
            # Export du rapport (optionnel)
            if options['export_report']:
                self._export_report(report, options['export_report'])
            
            # Audit log
            self._log_optimization_audit(options)
            
            if self.verbose:
                self.stdout.write(
                    self.style.SUCCESS(f'‚úÖ Optimisation termin√©e en {duration.total_seconds():.2f}s')
                )
            
        except Exception as e:
            logger.error(f"Erreur lors de l'optimisation des r√®gles: {str(e)}")
            raise CommandError(f'Erreur lors de l\'optimisation: {str(e)}')
    
    def _get_rules_to_analyze(self, rule_type: str) -> List[SecurityRuleModel]:
        """R√©cup√®re les r√®gles √† analyser selon le type sp√©cifi√©."""
        queryset = SecurityRuleModel.objects.filter(is_active=True)
        
        if rule_type != 'all':
            queryset = queryset.filter(rule_type=rule_type)
        
        rules = list(queryset.order_by('priority', 'created_at'))
        
        if self.verbose:
            self.stdout.write(f"üìä {len(rules)} r√®gles trouv√©es pour l'analyse")
        
        return rules
    
    def _detect_conflicts(self, rules: List[SecurityRuleModel], rule_type: str) -> List:
        """D√©tecte les conflits entre les r√®gles."""
        all_conflicts = []
        
        if rule_type == 'all':
            # Analyser par type de r√®gle
            for rtype in ['firewall', 'ids', 'access_control']:
                type_rules = [r for r in rules if r.rule_type == rtype]
                if type_rules:
                    conflicts = self.conflict_use_case.analyze_ruleset_conflicts(rtype)
                    all_conflicts.extend(conflicts)
        else:
            # Analyser un type sp√©cifique
            conflicts = self.conflict_use_case.analyze_ruleset_conflicts(rule_type)
            all_conflicts.extend(conflicts)
        
        if self.verbose:
            conflict_types = {}
            for conflict in all_conflicts:
                ctype = getattr(conflict, 'conflict_type', 'unknown')
                conflict_types[ctype] = conflict_types.get(ctype, 0) + 1
            
            self.stdout.write(f"‚ö†Ô∏è  {len(all_conflicts)} conflits d√©tect√©s:")
            for ctype, count in conflict_types.items():
                self.stdout.write(f"   - {ctype}: {count}")
        
        return all_conflicts
    
    def _validate_rules_via_docker(self, rules: List[SecurityRuleModel]) -> Dict[str, Any]:
        """Valide les r√®gles via les services Docker."""
        validation_results = {
            'suricata': {'validated': 0, 'errors': 0},
            'fail2ban': {'validated': 0, 'errors': 0},
            'traffic_control': {'validated': 0, 'errors': 0}
        }
        
        for rule in rules:
            detector = conflict_detector_factory.get_detector(rule.rule_type)
            if not detector:
                continue
            
            try:
                # Validation sp√©cifique selon le type
                if rule.rule_type == 'firewall' and hasattr(detector, '_validate_rule_via_docker'):
                    result = detector._validate_rule_via_docker(rule.content)
                    service = 'traffic_control'
                elif rule.rule_type == 'ids' and hasattr(detector, '_validate_rule_via_docker'):
                    result = detector._validate_rule_via_docker(rule.content)
                    service = 'suricata'
                elif rule.rule_type == 'access_control' and hasattr(detector, '_validate_rule_via_docker'):
                    result = detector._validate_rule_via_docker(rule.content)
                    service = 'fail2ban'
                else:
                    continue
                
                if result.get('valid', True):
                    validation_results[service]['validated'] += 1
                else:
                    validation_results[service]['errors'] += 1
                    if self.verbose:
                        self.stdout.write(
                            self.style.WARNING(
                                f"‚ùå R√®gle {rule.id} invalide: {result.get('error', 'Erreur inconnue')}"
                            )
                        )
                        
            except Exception as e:
                if self.verbose:
                    self.stdout.write(
                        self.style.ERROR(f"üî• Erreur lors de la validation de la r√®gle {rule.id}: {str(e)}")
                    )
        
        if self.verbose:
            total_validated = sum(r['validated'] for r in validation_results.values())
            total_errors = sum(r['errors'] for r in validation_results.values())
            self.stdout.write(f"‚úÖ {total_validated} r√®gles valid√©es, ‚ùå {total_errors} erreurs")
        
        return validation_results
    
    def _analyze_performance(self, rules: List[SecurityRuleModel]) -> Dict[str, Any]:
        """Analyse les performances des r√®gles."""
        try:
            # Collecter les m√©triques Docker
            current_metrics = self.metrics_collector.collect_all_metrics()
            
            performance_analysis = {
                'total_rules': len(rules),
                'performance_score': 0.0,
                'bottlenecks': [],
                'improvements_available': False,
                'recommendations': []
            }
            
            # Analyser l'impact de chaque r√®gle
            total_impact = 0.0
            high_impact_rules = []
            
            for rule in rules:
                try:
                    impact = self.metrics_calculator.calculate_rule_impact(
                        rule_type=rule.rule_type,
                        rule_content=rule.content,
                        current_metrics=current_metrics
                    )
                    
                    # √âvaluer l'impact performance
                    if hasattr(impact, 'performance_metrics'):
                        perf_metrics = impact.performance_metrics
                        rule_impact = (
                            getattr(perf_metrics, 'cpu_usage_percent', 0) * 0.4 +
                            getattr(perf_metrics, 'memory_usage_mb', 0) / 1000 * 0.3 +
                            getattr(perf_metrics, 'network_latency_ms', 0) / 1000 * 0.3
                        )
                        
                        total_impact += rule_impact
                        
                        if rule_impact > 2.0:  # Seuil d'impact √©lev√©
                            high_impact_rules.append({
                                'rule_id': rule.id,
                                'rule_name': rule.name,
                                'impact_score': rule_impact,
                                'type': rule.rule_type
                            })
                            
                except Exception as e:
                    if self.verbose:
                        self.stdout.write(
                            self.style.WARNING(f"‚ö†Ô∏è  Erreur analyse performance r√®gle {rule.id}: {str(e)}")
                        )
            
            # Calculer le score global
            if rules:
                avg_impact = total_impact / len(rules)
                performance_analysis['performance_score'] = max(0.0, 10.0 - avg_impact)
                
                if avg_impact > 2.0:
                    performance_analysis['improvements_available'] = True
                    performance_analysis['recommendations'].append(
                        "Optimisation recommand√©e: impact performance √©lev√© d√©tect√©"
                    )
            
            # Identifier les goulots d'√©tranglement
            if high_impact_rules:
                performance_analysis['bottlenecks'] = high_impact_rules[:5]  # Top 5
                performance_analysis['recommendations'].append(
                    f"{len(high_impact_rules)} r√®gles √† fort impact identifi√©es"
                )
            
            if self.verbose:
                self.stdout.write(
                    f"üìà Score de performance: {performance_analysis['performance_score']:.2f}/10"
                )
                if high_impact_rules:
                    self.stdout.write(f"‚ö†Ô∏è  {len(high_impact_rules)} r√®gles √† fort impact d√©tect√©es")
            
            return performance_analysis
            
        except Exception as e:
            if self.verbose:
                self.stdout.write(
                    self.style.WARNING(f"‚ö†Ô∏è  Erreur lors de l'analyse de performance: {str(e)}")
                )
            return {'error': str(e)}
    
    def _auto_resolve_conflicts(self, conflicts: List, dry_run: bool = False) -> int:
        """R√©sout automatiquement les conflits simples."""
        resolved_count = 0
        
        for conflict in conflicts:
            conflict_type = getattr(conflict, 'conflict_type', '')
            
            # R√©solution automatique pour les types simples
            if conflict_type == 'redundant':
                if self._resolve_redundant_conflict(conflict, dry_run):
                    resolved_count += 1
            elif conflict_type == 'generalization':
                if self._resolve_generalization_conflict(conflict, dry_run):
                    resolved_count += 1
        
        if self.verbose and resolved_count > 0:
            action = "seraient r√©solus" if dry_run else "r√©solus"
            self.stdout.write(f"üîß {resolved_count} conflits {action} automatiquement")
        
        return resolved_count
    
    def _resolve_redundant_conflict(self, conflict, dry_run: bool = False) -> bool:
        """R√©sout un conflit de redondance en supprimant la r√®gle la moins prioritaire."""
        try:
            rule1_id = getattr(conflict, 'rule1_id', None)
            rule2_id = getattr(conflict, 'rule2_id', None)
            
            if not rule1_id or not rule2_id:
                return False
            
            # R√©cup√©rer les r√®gles
            rule1 = SecurityRuleModel.objects.filter(id=rule1_id).first()
            rule2 = SecurityRuleModel.objects.filter(id=rule2_id).first()
            
            if not rule1 or not rule2:
                return False
            
            # D√©terminer quelle r√®gle supprimer (la moins prioritaire ou la plus r√©cente)
            if rule1.priority > rule2.priority:
                rule_to_remove = rule1
                rule_to_keep = rule2
            elif rule1.priority < rule2.priority:
                rule_to_remove = rule2
                rule_to_keep = rule1
            else:
                # M√™me priorit√©, supprimer la plus r√©cente
                rule_to_remove = rule1 if rule1.created_at > rule2.created_at else rule2
                rule_to_keep = rule2 if rule1.created_at > rule2.created_at else rule1
            
            if not dry_run:
                # D√©sactiver plut√¥t que supprimer pour pr√©server l'historique
                rule_to_remove.is_active = False
                rule_to_remove.save()
                
                # Ajouter une note dans les m√©tadonn√©es
                metadata = rule_to_remove.metadata or {}
                metadata['deactivated_reason'] = f'Redundant with rule {rule_to_keep.id}'
                metadata['deactivated_at'] = timezone.now().isoformat()
                rule_to_remove.metadata = metadata
                rule_to_remove.save()
            
            if self.verbose:
                action = "serait d√©sactiv√©e" if dry_run else "d√©sactiv√©e"
                self.stdout.write(
                    f"üîß R√®gle redondante {rule_to_remove.id} {action} (redondante avec {rule_to_keep.id})"
                )
            
            return True
            
        except Exception as e:
            if self.verbose:
                self.stdout.write(
                    self.style.ERROR(f"‚ùå Erreur lors de la r√©solution du conflit redondant: {str(e)}")
                )
            return False
    
    def _resolve_generalization_conflict(self, conflict, dry_run: bool = False) -> bool:
        """R√©sout un conflit de g√©n√©ralisation en supprimant la r√®gle sp√©cifique."""
        try:
            rule1_id = getattr(conflict, 'rule1_id', None)
            rule2_id = getattr(conflict, 'rule2_id', None)
            
            if not rule1_id or not rule2_id:
                return False
            
            # Dans un conflit de g√©n√©ralisation, une r√®gle est plus g√©n√©rale que l'autre
            # Selon la logique du d√©tecteur, rule1 est g√©n√©ralement la plus g√©n√©rale
            rule_to_remove = SecurityRuleModel.objects.filter(id=rule2_id).first()
            rule_to_keep = SecurityRuleModel.objects.filter(id=rule1_id).first()
            
            if not rule_to_remove or not rule_to_keep:
                return False
            
            if not dry_run:
                rule_to_remove.is_active = False
                rule_to_remove.save()
                
                metadata = rule_to_remove.metadata or {}
                metadata['deactivated_reason'] = f'Generalized by rule {rule_to_keep.id}'
                metadata['deactivated_at'] = timezone.now().isoformat()
                rule_to_remove.metadata = metadata
                rule_to_remove.save()
            
            if self.verbose:
                action = "serait d√©sactiv√©e" if dry_run else "d√©sactiv√©e"
                self.stdout.write(
                    f"üîß R√®gle sp√©cifique {rule_to_remove.id} {action} (g√©n√©ralis√©e par {rule_to_keep.id})"
                )
            
            return True
            
        except Exception as e:
            if self.verbose:
                self.stdout.write(
                    self.style.ERROR(f"‚ùå Erreur lors de la r√©solution du conflit de g√©n√©ralisation: {str(e)}")
                )
            return False
    
    def _consolidate_redundant_rules(self, conflicts: List, dry_run: bool = False) -> int:
        """Consolide les r√®gles redondantes identiques."""
        consolidated_count = 0
        
        # Identifier les groupes de r√®gles redondantes
        redundant_groups = self._identify_redundant_groups(conflicts)
        
        for group in redundant_groups:
            if len(group) > 1:
                if self._consolidate_rule_group(group, dry_run):
                    consolidated_count += len(group) - 1  # -1 car on garde une r√®gle
        
        if self.verbose and consolidated_count > 0:
            action = "seraient consolid√©es" if dry_run else "consolid√©es"
            self.stdout.write(f"üì¶ {consolidated_count} r√®gles {action}")
        
        return consolidated_count
    
    def _identify_redundant_groups(self, conflicts: List) -> List[List[int]]:
        """Identifie les groupes de r√®gles redondantes."""
        redundant_pairs = []
        
        for conflict in conflicts:
            if getattr(conflict, 'conflict_type', '') == 'redundant':
                rule1_id = getattr(conflict, 'rule1_id', None)
                rule2_id = getattr(conflict, 'rule2_id', None)
                if rule1_id and rule2_id:
                    redundant_pairs.append((rule1_id, rule2_id))
        
        # Grouper les paires en groupes connect√©s
        groups = []
        processed_rules = set()
        
        for rule1_id, rule2_id in redundant_pairs:
            if rule1_id in processed_rules or rule2_id in processed_rules:
                # Ajouter aux groupes existants
                for group in groups:
                    if rule1_id in group or rule2_id in group:
                        group.update([rule1_id, rule2_id])
                        processed_rules.update([rule1_id, rule2_id])
                        break
            else:
                # Cr√©er un nouveau groupe
                new_group = {rule1_id, rule2_id}
                groups.append(new_group)
                processed_rules.update([rule1_id, rule2_id])
        
        return [list(group) for group in groups]
    
    def _consolidate_rule_group(self, rule_ids: List[int], dry_run: bool = False) -> bool:
        """Consolide un groupe de r√®gles redondantes."""
        try:
            rules = SecurityRuleModel.objects.filter(id__in=rule_ids, is_active=True)
            if len(rules) < 2:
                return False
            
            # Choisir la r√®gle √† conserver (la plus ancienne avec la meilleure priorit√©)
            main_rule = min(rules, key=lambda r: (r.priority, r.created_at))
            rules_to_merge = [r for r in rules if r.id != main_rule.id]
            
            if not dry_run:
                # Mettre √† jour la r√®gle principale avec les m√©tadonn√©es consolid√©es
                consolidated_metadata = main_rule.metadata or {}
                consolidated_metadata['consolidated_from'] = [r.id for r in rules_to_merge]
                consolidated_metadata['consolidated_at'] = timezone.now().isoformat()
                
                # Ajouter les descriptions des r√®gles merg√©es
                descriptions = [main_rule.description or '']
                descriptions.extend([r.description or '' for r in rules_to_merge if r.description])
                consolidated_metadata['merged_descriptions'] = descriptions
                
                main_rule.metadata = consolidated_metadata
                main_rule.save()
                
                # D√©sactiver les autres r√®gles
                for rule in rules_to_merge:
                    rule.is_active = False
                    rule_metadata = rule.metadata or {}
                    rule_metadata['consolidated_into'] = main_rule.id
                    rule_metadata['consolidated_at'] = timezone.now().isoformat()
                    rule.metadata = rule_metadata
                    rule.save()
            
            if self.verbose:
                action = "seraient consolid√©es" if dry_run else "consolid√©es"
                rule_ids_str = ', '.join(str(r.id) for r in rules_to_merge)
                self.stdout.write(
                    f"üì¶ R√®gles {rule_ids_str} {action} dans la r√®gle {main_rule.id}"
                )
            
            return True
            
        except Exception as e:
            if self.verbose:
                self.stdout.write(
                    self.style.ERROR(f"‚ùå Erreur lors de la consolidation: {str(e)}")
                )
            return False
    
    def _optimize_rule_performance(self, rules: List[SecurityRuleModel], dry_run: bool = False) -> int:
        """Optimise les performances des r√®gles."""
        optimized_count = 0
        
        for rule in rules:
            # Optimisations basiques selon le type de r√®gle
            optimizations = self._identify_rule_optimizations(rule)
            
            if optimizations and not dry_run:
                # Appliquer les optimisations
                if self._apply_rule_optimizations(rule, optimizations):
                    optimized_count += 1
            elif optimizations:
                optimized_count += 1
        
        if self.verbose and optimized_count > 0:
            action = "seraient optimis√©es" if dry_run else "optimis√©es"
            self.stdout.write(f"‚ö° {optimized_count} r√®gles {action} pour les performances")
        
        return optimized_count
    
    def _identify_rule_optimizations(self, rule: SecurityRuleModel) -> List[Dict[str, Any]]:
        """Identifie les optimisations possibles pour une r√®gle."""
        optimizations = []
        
        if rule.rule_type == 'firewall':
            # Optimisations pour r√®gles firewall
            if 'any' in rule.content.lower() and rule.content.count('any') > 2:
                optimizations.append({
                    'type': 'specificity_improvement',
                    'description': 'Trop de wildcards "any", sp√©cifier davantage'
                })
        
        elif rule.rule_type == 'ids':
            # Optimisations pour r√®gles IDS
            if rule.content.count('content:') > 5:
                optimizations.append({
                    'type': 'content_reduction',
                    'description': 'Trop de patterns de contenu, optimiser les expressions'
                })
            
            if 'pcre:' in rule.content and len(rule.content) > 500:
                optimizations.append({
                    'type': 'regex_optimization',
                    'description': 'Expression r√©guli√®re complexe, simplifier si possible'
                })
        
        # Optimisations g√©n√©rales
        if not rule.metadata or not rule.metadata.get('last_optimized'):
            optimizations.append({
                'type': 'metadata_update',
                'description': 'Ajouter m√©tadonn√©es d\'optimisation'
            })
        
        return optimizations
    
    def _apply_rule_optimizations(self, rule: SecurityRuleModel, optimizations: List[Dict[str, Any]]) -> bool:
        """Applique les optimisations √† une r√®gle."""
        try:
            modified = False
            
            for optimization in optimizations:
                if optimization['type'] == 'metadata_update':
                    metadata = rule.metadata or {}
                    metadata['last_optimized'] = timezone.now().isoformat()
                    metadata['optimizations_applied'] = [opt['type'] for opt in optimizations]
                    rule.metadata = metadata
                    modified = True
                
                # Ajouter d'autres types d'optimisations selon les besoins
            
            if modified:
                rule.save()
                
                if self.verbose:
                    opt_types = [opt['type'] for opt in optimizations]
                    self.stdout.write(
                        f"‚ö° R√®gle {rule.id} optimis√©e: {', '.join(opt_types)}"
                    )
            
            return modified
            
        except Exception as e:
            if self.verbose:
                self.stdout.write(
                    self.style.ERROR(f"‚ùå Erreur lors de l'optimisation de la r√®gle {rule.id}: {str(e)}")
                )
            return False
    
    def _generate_optimization_report(
        self, 
        rules: List[SecurityRuleModel], 
        conflicts: List, 
        performance_analysis: Optional[Dict[str, Any]], 
        duration: timedelta
    ) -> Dict[str, Any]:
        """G√©n√®re un rapport complet d'optimisation."""
        report = {
            'optimization_summary': {
                'timestamp': timezone.now().isoformat(),
                'duration_seconds': duration.total_seconds(),
                'rules_analyzed': len(rules),
                'optimization_stats': self.optimization_stats.copy()
            },
            'conflict_analysis': {
                'total_conflicts': len(conflicts),
                'conflicts_by_type': {},
                'critical_conflicts': 0,
                'resolved_conflicts': self.optimization_stats['conflicts_resolved']
            },
            'performance_analysis': performance_analysis,
            'recommendations': [],
            'next_steps': []
        }
        
        # Analyser les conflits par type
        for conflict in conflicts:
            ctype = getattr(conflict, 'conflict_type', 'unknown')
            severity = getattr(conflict, 'severity', 'unknown')
            
            report['conflict_analysis']['conflicts_by_type'][ctype] = \
                report['conflict_analysis']['conflicts_by_type'].get(ctype, 0) + 1
            
            if severity == 'critical':
                report['conflict_analysis']['critical_conflicts'] += 1
        
        # G√©n√©rer des recommandations
        if report['conflict_analysis']['critical_conflicts'] > 0:
            report['recommendations'].append(
                f"üö® {report['conflict_analysis']['critical_conflicts']} conflits critiques n√©cessitent une attention imm√©diate"
            )
        
        if performance_analysis and performance_analysis.get('improvements_available'):
            report['recommendations'].append(
                "‚ö° Optimisations de performance disponibles"
            )
        
        unresolved_conflicts = len(conflicts) - self.optimization_stats['conflicts_resolved']
        if unresolved_conflicts > 0:
            report['next_steps'].append(
                f"Examiner manuellement {unresolved_conflicts} conflits non r√©solus"
            )
        
        if self.optimization_stats['rules_optimized'] > 0:
            report['next_steps'].append(
                "Surveiller les performances apr√®s optimisation"
            )
        
        return report
    
    def _display_results(self, report: Dict[str, Any], output_format: str):
        """Affiche les r√©sultats de l'optimisation."""
        if output_format == 'json':
            self.stdout.write(json.dumps(report, indent=2, ensure_ascii=False))
        else:
            # Affichage texte format√©
            self.stdout.write(self.style.SUCCESS("\nüîß RAPPORT D'OPTIMISATION DES R√àGLES DE S√âCURIT√â"))
            self.stdout.write("=" * 60)
            
            # R√©sum√©
            summary = report['optimization_summary']
            self.stdout.write(f"\nüìä R√âSUM√â")
            self.stdout.write(f"   Dur√©e d'ex√©cution: {summary['duration_seconds']:.2f}s")
            self.stdout.write(f"   R√®gles analys√©es: {summary['rules_analyzed']}")
            self.stdout.write(f"   Conflits d√©tect√©s: {summary['optimization_stats']['conflicts_detected']}")
            self.stdout.write(f"   Conflits r√©solus: {summary['optimization_stats']['conflicts_resolved']}")
            self.stdout.write(f"   R√®gles optimis√©es: {summary['optimization_stats']['rules_optimized']}")
            self.stdout.write(f"   R√®gles consolid√©es: {summary['optimization_stats']['rules_consolidated']}")
            
            # Analyse des conflits
            conflict_analysis = report['conflict_analysis']
            if conflict_analysis['total_conflicts'] > 0:
                self.stdout.write(f"\n‚ö†Ô∏è  CONFLITS D√âTECT√âS")
                for ctype, count in conflict_analysis['conflicts_by_type'].items():
                    self.stdout.write(f"   {ctype}: {count}")
                
                if conflict_analysis['critical_conflicts'] > 0:
                    self.stdout.write(
                        self.style.ERROR(f"   üö® Conflits critiques: {conflict_analysis['critical_conflicts']}")
                    )
            
            # Performance
            if report['performance_analysis']:
                perf = report['performance_analysis']
                self.stdout.write(f"\nüìà PERFORMANCE")
                self.stdout.write(f"   Score global: {perf.get('performance_score', 'N/A')}")
                
                if perf.get('bottlenecks'):
                    self.stdout.write(f"   Goulots d'√©tranglement: {len(perf['bottlenecks'])}")
            
            # Recommandations
            if report['recommendations']:
                self.stdout.write(f"\nüí° RECOMMANDATIONS")
                for rec in report['recommendations']:
                    self.stdout.write(f"   ‚Ä¢ {rec}")
            
            # Prochaines √©tapes
            if report['next_steps']:
                self.stdout.write(f"\nüîÑ PROCHAINES √âTAPES")
                for step in report['next_steps']:
                    self.stdout.write(f"   ‚Ä¢ {step}")
            
            self.stdout.write("\n" + "=" * 60)
    
    def _export_report(self, report: Dict[str, Any], file_path: str):
        """Exporte le rapport vers un fichier."""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            self.stdout.write(
                self.style.SUCCESS(f"üìÑ Rapport export√© vers: {file_path}")
            )
            
        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f"‚ùå Erreur lors de l'export: {str(e)}")
            )
    
    def _log_optimization_audit(self, options: Dict[str, Any]):
        """Enregistre l'audit de l'optimisation."""
        try:
            audit_details = {
                'command': 'optimize_security_rules',
                'options': {k: v for k, v in options.items() if k != 'verbosity'},
                'stats': self.optimization_stats.copy(),
                'timestamp': timezone.now().isoformat()
            }
            
            AuditLogModel.objects.create(
                action='optimization',
                target_type='security_rules',
                target_id=0,  # Global optimization
                user_id=None,  # System command
                ip_address='127.0.0.1',
                user_agent='Django Management Command',
                details=audit_details
            )
            
        except Exception as e:
            if self.verbose:
                self.stdout.write(
                    self.style.WARNING(f"‚ö†Ô∏è  Erreur lors de l'audit: {str(e)}")
                )
    
    def _log_step(self, step_name: str):
        """Log une √©tape de l'optimisation."""
        if self.verbose:
            self.stdout.write(f"\nüîÑ {step_name}...")